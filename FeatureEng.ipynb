{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Name: Christian Clarke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: import the modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.feature_selection import RFE, SelectKBest, chi2\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21a8fb9e048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "#TODO: provide the file path or URL to the dataset `trip_data.csv`\n",
    "file_path   = 'https://raw.githubusercontent.com/clarkech95/decision-trees-pt2/master/trip_data.csv' \n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.groupby('tip').apply(lambda x: x.sample(sample_size))\n",
    "df = df.sample(frac=1) #shuffle the dataframe rows\n",
    "\n",
    "#visualize the sample data, which contain 1000 observations from each class\n",
    "df.tip.value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a new feature called `trip_duration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new feature called trip_duration and add it to the dataframe\n",
    "\n",
    "#TODO: Calculate the difference between `lpep_pickup_datetime` and `lpep_dropoff_datetime` in seconds (this should be a positive number).\n",
    "#TODO: store the elapsed time (from above) into the dataframe.\n",
    "\n",
    "df['trip_duration'] = (pd.to_datetime(df['lpep_dropoff_datetime']) - pd.to_datetime(df['lpep_pickup_datetime']))\n",
    "df['trip_duration'] = df['trip_duration'].apply(lambda x: x.total_seconds())\n",
    "\n",
    "#Remove the `lpep_pickup_datetime` and `lpep_dropoff_datetime` from the dataframe.\n",
    "del df['lpep_dropoff_datetime']\n",
    "del df['lpep_pickup_datetime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Encode the labels in the `tip` field.\n",
    "Use the [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) in scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tip             \n",
       "no-tip    127605    2\n",
       "good      42876     0\n",
       "no-tip    49939     2\n",
       "standard  81621     3\n",
       "          124547    3\n",
       "no-tip    140047    2\n",
       "low       121122    1\n",
       "          136623    1\n",
       "no-tip    84661     2\n",
       "low       22218     1\n",
       "good      113750    0\n",
       "low       34043     1\n",
       "          92755     1\n",
       "good      21439     0\n",
       "no-tip    25752     2\n",
       "standard  6652      3\n",
       "good      79378     0\n",
       "standard  12703     3\n",
       "          10658     3\n",
       "          21291     3\n",
       "low       4713      1\n",
       "good      126486    0\n",
       "low       114073    1\n",
       "good      151567    0\n",
       "low       72183     1\n",
       "good      76003     0\n",
       "low       61562     1\n",
       "          57180     1\n",
       "standard  108519    3\n",
       "good      155692    0\n",
       "                   ..\n",
       "          3836      0\n",
       "low       86482     1\n",
       "good      124902    0\n",
       "no-tip    793       2\n",
       "good      134595    0\n",
       "no-tip    42918     2\n",
       "standard  90053     3\n",
       "          149783    3\n",
       "good      99610     0\n",
       "low       79529     1\n",
       "no-tip    618       2\n",
       "good      143405    0\n",
       "no-tip    18933     2\n",
       "good      169400    0\n",
       "          158480    0\n",
       "standard  144116    3\n",
       "no-tip    46611     2\n",
       "standard  69213     3\n",
       "          52229     3\n",
       "good      48516     0\n",
       "          80367     0\n",
       "no-tip    171551    2\n",
       "standard  71582     3\n",
       "          82548     3\n",
       "          87873     3\n",
       "good      54706     0\n",
       "low       108001    1\n",
       "no-tip    116528    2\n",
       "standard  152294    3\n",
       "no-tip    30898     2\n",
       "Name: tip, Length: 4000, dtype: int32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode the class labels\n",
    "\n",
    "#TODO: Initialize the `LabelEncoder`\n",
    "le = LabelEncoder()\n",
    "\n",
    "#TODO: Fit the `LabelEncoder` to the class labels in the 'tip' field\n",
    "le.fit(pd.unique(df['tip']))\n",
    "\n",
    "#TODO: Use the `LabelEncoder` to encode class labels in the 'tip' field.\n",
    "df['tip'] = le.transform(df['tip'])\n",
    "\n",
    "#TODO: Display the encoded labels\n",
    "df['tip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the input features and the class labels to variables 'X' and 'y' respectively.\n",
    "\n",
    "X = df.iloc[:,np.r_[0:14,15]].to_numpy() #TODO: assign the input features (in the dataframe) to a variable 'X'.\n",
    "y = df.iloc[:,14].tolist() #TODO: assign the encoded labels to a variale 'y'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Univariate feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection): use [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) to select the seven highest scoring features (i.e. k=7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PULocationID',\n",
       " 'DOLocationID',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'tip']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Univariate feature selection using SelectKBest.\n",
    "\n",
    "k_best   = SelectKBest(score_func= chi2 , #TODO choose either chi2 or f_classif\n",
    "                     k= 7#TODO: enter the value for k\n",
    "                    )\n",
    "\n",
    "k_best   = k_best.fit(X, y)\n",
    "\n",
    "features = k_best.transform(X)\n",
    "\n",
    "#TODO: display the names of the selected features below\n",
    "\n",
    "filter_one = [j for i, j in zip(k_best.get_support(), df.columns) if i == True]\n",
    "filter_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the top seven features using [Support Vector Machines (SVM)](https://scikit-learn.org/stable/modules/svm.html) and [Recursive feature elimination (RFE)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE).\n",
    "\n",
    "Here is an example on using [SVMs for classification](https://scikit-learn.org/stable/modules/svm.html#classification) and [RFE](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py) in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fare_amount',\n",
       " 'extra',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svm_estimator = svm.SVC(kernel='linear') #initialize the SVM estimator\n",
    "\n",
    "#TODO: Recursive Feature Elimination\n",
    "rfe = RFE(estimator= svm_estimator, #TODO: provide the SVM estimator \n",
    "          verbose=1, \n",
    "          n_features_to_select= 7 #TODO: enter the number of required features\n",
    "         )\n",
    "\n",
    "rfe = rfe.fit(X, y) #Note: this may take a few minutes to complete\n",
    "\n",
    "#TODO: display the names of the selected features below\n",
    "wrapper_one = [j for i,j in zip(rfe.get_support(), df.columns) if i == True]\n",
    "wrapper_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results\n",
    "Compare and contrast the selected features from the filter and wrapper methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both produced compelling results however, I feel the attributes decided by the filter method are better suited to this problem than the wrapper method's. Interestingly enough, they do share three of the same attributes, those being fare amount, total amount and congestion surchage. However, the wrapper method then proceeds to select several features which show little variance throughout our dataset so I'm curious as to why they are interpreted to have so much value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filter_one)\n",
    "print(wrapper_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Standardize the input features\n",
    "Use the [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) in [scikit-learn's preprocessing module](https://scikit-learn.org/stable/modules/preprocessing.html) to scale the input features between 0.1 and 0.9 (i.e. the min and max respectively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the input features\n",
    "#TODO: provide the min and max values for the features\n",
    "scaler = MinMaxScaler(feature_range= (0.1,0.9)\n",
    "                     )\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "['VendorID', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'extra', 'congestion_surcharge']\n",
      "['RatecodeID', 'trip_distance', 'fare_amount', 'tolls_amount', 'total_amount', 'congestion_surcharge', 'tip']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform the SelectKBest and Recursive Feature Elimination steps using the standardized data\n",
    "k_best_scaled = SelectKBest(score_func= chi2 , #TODO choose either chi2 or f_classif\n",
    "                     k= 7#TODO: enter the value for k\n",
    "                    )\n",
    "k_best_scaled = k_best.fit(X, y)\n",
    "rfe_scaled = RFE(estimator= svm_estimator, #TODO: provide the SVM estimator \n",
    "          verbose=1, \n",
    "          n_features_to_select= 7 #TODO: enter the number of required features\n",
    "         )\n",
    "rfe_scaled = rfe.fit(X, y)\n",
    "\n",
    "#TODO: display the names of the selected features for each method\n",
    "filter_scaled = [j for i, j in zip(k_best.get_support(), df.columns) if i == True]\n",
    "wrapper_scaled = [j for i,j in zip(rfe.get_support(), df.columns) if i == True]\n",
    "print(filter_scaled)\n",
    "print(wrapper_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results\n",
    "Compare and contrast the scaled features from the filter and wrapper methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough there wasn't too much difference in either of the methods. The Filter method had two replacements and the Wrapper method had three replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'total_amount', 'congestion_surcharge', 'tip']\n",
      "['VendorID', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'extra', 'congestion_surcharge']\n",
      "['fare_amount', 'extra', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'trip_type', 'congestion_surcharge']\n",
      "['RatecodeID', 'trip_distance', 'fare_amount', 'tolls_amount', 'total_amount', 'congestion_surcharge', 'tip']\n"
     ]
    }
   ],
   "source": [
    "#TODO: Compare and contrast the results\n",
    "print(filter_one)\n",
    "print(filter_scaled)\n",
    "print(wrapper_one)\n",
    "print(wrapper_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Explain the effect of standardizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data helps to put every feature on the same scale. The issue with most datasets, this one included is that often different features are subject to different ranges. For example, trip distance values range from about 0 to about 8 whereas total amount ranges from about 3 to about 110. When this happens it can cause confusion for our algorithms as quite a few algorithms are affected by the magnitude of the values. This can lead them to falsely thinking that one feature is more important than another simply due to the size of the values within that feature. Thus when we standardize the data we see the algorithms realize the importance of certain features while dismissing others, we can think of standization as leveling the playing field for all of the features. It also helps speed up some of the calculations as an added bonus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7:  Observations about the selected features\n",
    "Based on your observations about the features, did the algorithm identify meaningful features and did they align with your previous intuition about the data (Recall the data wrangling exercise)? Did it identify any features that you would not have otherwise considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the algorithms did were fairly accurate with what they depicted as being meaningful features. For the most part it did fall in-line with most of my assumptions from the data wrangling exercise. Features like cogestion surcharge, extra, and improvement surchage caught me by surprise. Initially I would believe that these would fall into the category of being strongly correlated to total amount which would be a roll up of all of these different charges. That being said with the features available it makes sense that they would end up in the top 7. I do however feel that if we were to drop the number of features we're looking for these would be the first ones out. Also interesting that the trip duration didn't make it in as a strong enough feature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
